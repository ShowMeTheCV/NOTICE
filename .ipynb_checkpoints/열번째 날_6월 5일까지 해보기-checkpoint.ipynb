{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오늘의 목표! (얼룩말 책 9장_개정판 10장)\n",
    "\n",
    "- 오늘은 opencv-python을 활용한 객체 검출을 알아봅니다.           \n",
    "- 텐서플로우가 적용된 단원이니만큼 이미 알고계시는 내용이 많으시겠지만,         \n",
    "- 여기에 적용된 이론적 배경을 바탕으로 보시면서 공부해보시면 또 다른 시각에서 opencv를 활용해 볼 수 있을 것 같습니다.            \n",
    "\n",
    "### 목차\n",
    "\n",
    "1. 파이썬 객체검출이란?           \n",
    "2. 텐서플로우       \n",
    "3. 객체 검출         \n",
    "4. 정규 표현식        \n",
    "5. 객체 추적        \n",
    "6. 객체 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 파이썬 객체 검출이란?          \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 객체 검출 : 이미지에서 의미있는 객체를 탐지하는 알고리즘    \n",
    "     > 이미지 상에서 여러 객체를 식별하고 분석하는 것은 컴퓨터 비전의 궁극적인 목표 중 하나로서, 객체 검출을 통해 특정 사물이나 동물 등을 탐지해서 어떤 의미를 지닌 객체임을 인식하는 데 사용할 수 있다.        \n",
    "     \n",
    "- 검출은 __이미지에서 특정 클래스(사람, 건물, 대상)를 찾는 것__ 이며,      \n",
    "- 인식은 __검출된 대상이 어떤 객체인지 식별하는 것을 의미한다.__    \n",
    "> 사진에서 얼굴 검출(얼굴을 찾기) -> 얼굴 인식(누구인지 알기)            \n",
    "\n",
    "- 객체 검출은 머신러닝을 활용해 해당 객체만이 갖는 특징을 분석하여 찾는다. 각 객체가 갖는 특정 패턴을 찾아 학습을 진행한다.     \n",
    "- 객체 검출을 위한 단계는 __크게 세 가지로 나눌 수 있다.__       \n",
    "    ###### 1. Tensorflow object detection api의 __학습이 완료된 모델 그래프로 이미지 상에서 객체를 검출__\n",
    "        - COCO 데이터셋 활용\n",
    "    ###### 2. 라벨 맵을 불러와 검출되 객체의 이름을 표시한다. \n",
    "        - 프로토콜 버퍼 형식의 라벨 맵 파일 사용       \n",
    "        - 이 때, 프로토콜 버퍼를 읽기 위해 정규표현식을 활용해 내용을 리스트나 딕셔너리 형식으로 변환\n",
    "    ###### 3. 모델을 통해 검출된 정보와 라벨 맵의 정보를 매칭시켜 OpenCV 라이브러리로 이미지 위에 표시        \n",
    "    \n",
    "    \n",
    "- 각 단계는 다양하게 응용될 수 있다. ex) 학습이 완료된 모델 그래프를 활용하여 문자열 검출 등에 활용 등.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 텐서플로우              \n",
    "---      \n",
    "\n",
    "- 텐서플로우는 구글 브레인팀에서 제작하였으며, __아파치 라이선스 2.0__ 으로 공개되어 있다.\n",
    "    * 아파치 라이선스 : 해당 소프트웨어에서 파생된 프로그램을 제작할 수 있으며, 저작권을 양도, 전송할 수 있는 라이선스      \n",
    "    \n",
    "- 텐서플로우는 파이썬의 Numpy 라이브러리를 활용하여 수학적인 계산을 자동으로 처리한다.      \n",
    "- 파이썬, C, C++, R 등에서 지원한다.      \n",
    "- CPU와 GPU를 활용해 연산을 진행한다.      \n",
    "- __텐서플로우는 NVIDIA 그래픽카드만 지원한다...!__     \n",
    "\n",
    "- 아나콘다를 활용해 텐서플로우를 설치한다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q1. tf.__version__ 명령을 통해 설치된 텐서플로우 버전을 확인해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텐서플로우 설치 후, __Tensorflow detection model zoo__ 를 활용할 수 있다.      \n",
    "- pre-trained된 모델 중 COCO 데이터셋으로 훈련된 모델을 사용해 예제를 진행해보자.     \n",
    "\n",
    "> Q2. 이 예제에서는 __SSD MobileNet v2 320x320__ 모델을 사용합니다. 해당 모델을 다운로드해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Q2는 여기를 참조 : SSD MobileNet v2 320x320를 다운로드합니다.](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q3. 마찬가지로, 객체 이름을 표시하기 위해 라벨 파일이 저장된 페이지를 방문하여 라벨 맵을 다운로드해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Q3는 여기를 참조 : mscoco_label_map.pbtxt를 다운로드합니다.](https://github.com/tensorflow/models/tree/master/research/object_detection/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 객체 검출              \n",
    "---\n",
    "\n",
    "- 훈련된 추론 모델을 읽어 파이썬에서 활용하려면 Tensorflow I/O 래퍼를 활용한다.      \n",
    "- 로컬을 비롯해 구글 클라우드나 하둡에서도 데이터를 읽어올 수 있다.    \n",
    "- I/O 래퍼는 텍스트, 이진(binary), 로우(raw) 파일을 불러올 수 있다.   \n",
    "\n",
    "- 텐서플로우 파일시스템 함수로 추론 모델 그래프 파일을 불러온다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q4. 예제 9.2(그래프 정의) 를 참조하여 그래프를 읽어오세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예제 9.2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습된 모델 그래프 파일은 이진(binary) 파일이므로 이진 모드로 열어서 읽는다.     \n",
    "- 따라서 텐서플로우 파일 시스템 모드로 rb(read, binary)를 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q5. 동영상을 읽어오고, 이를 불러온 그래프에 적용할 수 있도록 한다. (구판과 개정판의 소스코드가 다르므로, 개정판 깃헙을 참고하였습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")\n",
    "capture = cv2.VideoCapture(\"bird.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        break\n",
    "\n",
    "    #---구판에만 있는 resize코드(아마 개정판이 되며 동영상 자체 사이즈를 수정했나봅니다.)--\n",
    "    #input_img = cv2.resize(frame, (300,300))\n",
    "    #-----------------------------------------------------------------------\n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = tf.convert_to_tensor(input_img)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [위에서 사용한 동영상은 여기서 View raw를 눌러 다운받으세요.](https://github.com/wikibook/opencv4/blob/opencv4-rev/Chapter10/Example-10-03-Py/bird.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 예제를 통해, 불러운 동영상의 프레임마다 객체를 분석할 수 있도록 준비한다.        \n",
    "- 위의 소스코드에서 에러가 난다면, 아마 학습된 이미지와의 사이즈 차이 때문일 것이다.     \n",
    "(구판에서는 소스코드에 이미지 리사이즈 코드가 포함되어 있습니다.)    \n",
    "\n",
    "\n",
    "- 동영상에서 프레임을 불러와, 이 프레임을 추론할 이미지로 변경한다.      \n",
    "- 추론 모델은 RGB 채널, 320X320 크기에서 추론을 수행하므로 각각의 프레임을 이에 맞추어 변경해준다.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   output_dict = model.signatures[\"serving_default\"](input_tensor)\n",
    "\n",
    "    classes = output_dict[\"detection_classes\"][0]\n",
    "    scores = output_dict[\"detection_scores\"][0]\n",
    "    boxes = output_dict[\"detection_boxes\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기부터는 완전 설명이 다르기 때무네... 발표할 때 개정판을 가진 분들의 해설을 기다려봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q6. 아래의 full 버전 코드에 직접 주석을 달아보며 흐름을 파악해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")\n",
    "capture = cv2.VideoCapture(\"bird.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        break\n",
    "\n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = tf.convert_to_tensor(input_img)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    output_dict = model.signatures[\"serving_default\"](input_tensor)\n",
    "\n",
    "    classes = output_dict[\"detection_classes\"][0]\n",
    "    scores = output_dict[\"detection_scores\"][0]\n",
    "    boxes = output_dict[\"detection_boxes\"][0]\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    for idx, score in enumerate(scores):\n",
    "        if score > 0.7:\n",
    "            class_id = int(classes[idx])\n",
    "            box = boxes[idx]\n",
    "\n",
    "            x1 = int(box[1] * width)\n",
    "            y1 = int(box[0] * height)\n",
    "            x2 = int(box[3] * width)\n",
    "            y2 = int(box[2] * height)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), 255, 1)\n",
    "            cv2.putText(frame, str(class_id) + \":\" + str(float(score)), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 255, 255), 1)\n",
    "\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "    if cv2.waitKey(33) == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 정규표현식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 프로그래밍에서 사용하는 형식 언어           \n",
    "- 특정한 규칙을 가진 문자열을 검색, 분리, 치환하는 데 주로 활용됨      \n",
    "- 특정한 패턴과 일치하는 텍스트를 입력값에서 찾아 반환한다.      \n",
    "- __정규 표현식을 사용하지 않는 경우 매우 복잡한 코드를 작성해야 하지만, 정규표현식을 활용할 경우 코드가 매우 간결해지며 유사한 문자까지 일치시켜 검색할 수 있다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규표현식으로 라벨 맵 파일에서 특정 패턴을 찾으려면 __라벨 맵 파일의 구조를 알아야 한다.__  \n",
    "- 라벨 맵 파일 구조는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![라벨맵파일구조](./PostingPic/10_라벨맵파일구조.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- item 필드 안에 사전 형식의 name, id, display_name 으로 구성되어 있다.        \n",
    "- 라벨 맵 파일에서 필요한 값은 id와 display_name 이다.        \n",
    "\n",
    "- 따라서, 라벨 맵 파일에서 시각적으로 확인할 수 있는 이름을 가져오기 위해 파이썬에 내장된 파일시스템 함수를 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q7. 아래 예제를 실행시키고 결과를 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#라벨 맵 파일 불러오기 예제\n",
    "with open(\"mscoco_complete_label_map.pbtxt\", \"rt\") as f:\n",
    "    pb_classes = f.read().rstrip(\"\\n\").split(\"\\n\")\n",
    "\n",
    "    print(pb_classes[0])\n",
    "    print(pb_classes[1])\n",
    "    print(pb_classes[2])\n",
    "    print(pb_classes[3])\n",
    "    print(pb_classes[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이제 내부를 확인했으므로, __정규표현식 함수를 통해 파싱한다.__    \n",
    "- 정규표현식을 활용하기 위해 __import re__ 하여 패턴과 일치하는 문자열을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q8. re.findall() 함수와 re.search() 함수를 설명하는 공식 가이드를 찾아보고, 각각의 옵션을 설명해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q9. 아래의 예제(정규표현식을 활용해 입력 문자열에서 패턴 검출하기)를 수행하고, 주석을 달아보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "with open(\"mscoco_complete_label_map.pbtxt\", \"rt\") as f:\n",
    "    pb_classes = f.read().rstrip(\"\\n\").split(\"\\n\")\n",
    "    classes_label = dict()\n",
    "    \n",
    "    for i in range(0, len(pb_classes), 5):\n",
    "        pb_classId = int(re.findall(\"\\d+\", pb_classes[i + 2])[0])\n",
    "        pattern = 'display_name: \"(.*?)\"'\n",
    "        pb_text = re.search(pattern, pb_classes[i + 3])\n",
    "        classes_label[pb_classId] = pb_text.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정규 표현식의 패턴을 구성할 때, 일반적으로 패턴의 문자열은 로 문자열 표기법(raw string notation)으로 작성한다.      \n",
    "- 로 문자열 표기법은 문자열에 'r'을 포함하여 작성한다.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q10. 아래의 코드는 텐서플로우를 활용한 객체 검출의 전체 코드입니다. 전체 순서를 파악해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with open(\"mscoco_complete_label_map.pbtxt\", \"rt\") as f:\n",
    "    pb_classes = f.read().rstrip(\"\\n\").split(\"\\n\")\n",
    "    classes_label = dict()\n",
    "\n",
    "    for i in range(0, len(pb_classes), 5):\n",
    "        pb_classId = int(re.findall(\"\\d+\", pb_classes[i + 2])[0])\n",
    "        pattern = 'display_name: \"(.*?)\"'\n",
    "        pb_text = re.search(pattern, pb_classes[i + 3])\n",
    "        classes_label[pb_classId] = pb_text.group(1)\n",
    "\n",
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")\n",
    "capture = cv2.VideoCapture(\"bird.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        break\n",
    "\n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = tf.convert_to_tensor(input_img)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    output_dict = model.signatures[\"serving_default\"](input_tensor)\n",
    "\n",
    "    classes = output_dict[\"detection_classes\"][0]\n",
    "    scores = output_dict[\"detection_scores\"][0]\n",
    "    boxes = output_dict[\"detection_boxes\"][0]\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    for idx, score in enumerate(scores):\n",
    "        if score > 0.7:\n",
    "            class_id = int(classes[idx])\n",
    "            box = boxes[idx]\n",
    "\n",
    "            x1 = int(box[1] * width)\n",
    "            y1 = int(box[0] * height)\n",
    "            x2 = int(box[3] * width)\n",
    "            y2 = int(box[2] * height)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), 255, 1)\n",
    "            cv2.putText(frame, classes_label[class_id] + \":\" + str(float(score)), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 255, 255), 1)\n",
    "\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "    if cv2.waitKey(33) == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 객체 추적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이전까지의 스텝이 이미지 안의 __특정 객체를 검출하는 기술을 살펴봤다면, 이제는 객체의 움직임을 추적하는 알고리즘을 살펴보자.__          \n",
    "\n",
    "\n",
    "- 객체 추적은 영상에서 \n",
    "    1. 객체의 움직임      \n",
    "    2. 누적 경로        \n",
    "    3. 예상 경로        \n",
    "    4. 속도       \n",
    "    5. 속력\n",
    "    등을 확인하는 기술이다.     \n",
    "    \n",
    "    \n",
    "- 주요 추적 방식으로는        \n",
    "    1. 특징점 추적(Point Tracking) : 움직이는 객체의 특징점을 찾음         \n",
    "    2. 커널 기반 추적(Kernel based Tracking) : 일정 영역 내부의 움직임을 찾음     \n",
    "    3. 실루엣 기반 추적(Silhouette based Tracking) : 복잡한 형태를 단순화하여 움직임을 찾음       \n",
    "    등이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __광학 흐름(Optical Flow)를 통해 위와 같은 방식의 움직임을 검출할 수 있다.__       \n",
    "- 광학 흐름은 영상 내 물체의 움직임 패턴을 말한다. 이전 프레임과 다음 프레임 간 픽셀이 이동한 방향과 거리 분포이다.      \n",
    "- 밝기 변화가 거의 없고, 일정 블록 내의 모든 픽셀이 모두 같은 운동을 한다고 가정하여 움직임을 추정한다.     \n",
    "- 다른 방식으로는 특정 점만 사용하거나, 일정 블록 내의 움직임을 판단하여 감지하는 방식이 있다.  \n",
    "- __주로 동작 감지, 물체 추적, 구조 분석에 이용한다.__   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기서는 파네백 광학 흐름(다항식 확장 알고리즘)을 활용해, __각 픽셀의 이웃을 근사해 객체를 추적한다.__  \n",
    "- 밀집 광학 흐름(Dense optical flow) 중의 하나이며, 이미지의 모든 픽셀에 모션 벡터를 각각 할당한다.   \n",
    "- 특징 점에 집중하는 것이 아닌 모든 점에 대해 밀도가 높은 광학 흐름을 찾으므로, __정확도는 높지만 연산량이 많아진다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q11. 다른 밀집 광학 흐름 알고리즘도 있을까요? 밀집 광학 알고리즘에 대해 알아보고 요약하여 알려주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파네벡 광학 흐름 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [이 블로그 주인분이 설명을 잘 해놓으셨더군요](https://deep-learning-study.tistory.com/278)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파네벡 광학 흐름 함수는 8비트 단일 채널 이미지인 __이전 프레임과 다음 프레임을 사용하여 두 프레임 간의 광학 흐름을 계산한다.__           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![파네벡](./PostingPic/10_파네벡.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 따라서, 이전 프레임과 비교하여 객체의 움직임과 방향을 위와 같이 표시할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q12. 파네벡 광학 함수를 파이썬 공식 가이드에서 찾아, 옵션을 설명해보세요.(cv2.calcOpticalFlowFarneback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이전의 풀 버전 코드에서, 파네벡 광학 함수를 적용하기 위해 몇 줄을 추가해봅시다.\n",
    "# (주석처리)된 부분이 이전의 풀 버전과 비교하여 새로 추가된 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with open(\"mscoco_complete_label_map.pbtxt\", \"rt\") as f:\n",
    "    pb_classes = f.read().rstrip(\"\\n\").split(\"\\n\")\n",
    "    classes_label = dict()\n",
    "\n",
    "    for i in range(0, len(pb_classes), 5):\n",
    "        pb_classId = int(re.findall(\"\\d+\", pb_classes[i + 2])[0])\n",
    "        pattern = 'display_name: \"(.*?)\"'\n",
    "        pb_text = re.search(pattern, pb_classes[i + 3])\n",
    "        classes_label[pb_classId] = pb_text.group(1)\n",
    "\n",
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")\n",
    "capture = cv2.VideoCapture(\"bird.mp4\")\n",
    "#ret, prev_frame = capture.read()\n",
    "#prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    #ret, next_frame = capture.read()\n",
    "    #next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n",
    "    #frame = next_frame.copy()\n",
    "\n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        break\n",
    "\n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = tf.convert_to_tensor(input_img)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    output_dict = model.signatures[\"serving_default\"](input_tensor)\n",
    "\n",
    "    classes = output_dict[\"detection_classes\"][0]\n",
    "    scores = output_dict[\"detection_scores\"][0]\n",
    "    boxes = output_dict[\"detection_boxes\"][0]\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    for idx, score in enumerate(scores):\n",
    "        if score > 0.7:\n",
    "            class_id = int(classes[idx])\n",
    "            box = boxes[idx]\n",
    "\n",
    "            x1 = int(box[1] * width)\n",
    "            y1 = int(box[0] * height)\n",
    "            x2 = int(box[3] * width)\n",
    "            y2 = int(box[2] * height)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), 255, 1)\n",
    "            cv2.putText(frame, classes_label[class_id] + \":\" + str(float(score)), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 255, 255), 1)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, flow=None, pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.1, flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "\n",
    "    for i in range(x1, x2, 15):\n",
    "        for j in range(y1, y2, 15):\n",
    "            x = int(flow[j, i, 0])\n",
    "            y = int(flow[j, i, 1])\n",
    "\n",
    "            cv2.circle(frame, (i, j), 1, (0, 0, 255), 1)\n",
    "            cv2.line(frame, (i, j), (i + x, j + y), (0, 255, 255), 1)\n",
    "            cv2.circle(frame, (i + x, j + y), 1, (0, 255, 255), 2)\n",
    "\n",
    "    #prev_gray = next_gray.copy()\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "    if cv2.waitKey(33) == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q13. 위에서 주석처리된 함수는 어떤 기능을 하나요? 이 추가된 줄들은 어떤 역할을 하기 위해 추가되었나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파네벡 광학 흐름을 적용하여 객체 내부의 흐름을 시각적으로 표현한 소스코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with open(\"mscoco_complete_label_map.pbtxt\", \"rt\") as f:\n",
    "    pb_classes = f.read().rstrip(\"\\n\").split(\"\\n\")\n",
    "    classes_label = dict()\n",
    "\n",
    "    for i in range(0, len(pb_classes), 5):\n",
    "        pb_classId = int(re.findall(\"\\d+\", pb_classes[i + 2])[0])\n",
    "        pattern = 'display_name: \"(.*?)\"'\n",
    "        pb_text = re.search(pattern, pb_classes[i + 3])\n",
    "        classes_label[pb_classId] = pb_text.group(1)\n",
    "\n",
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")\n",
    "capture = cv2.VideoCapture(\"bird.mp4\")\n",
    "ret, prev_frame = capture.read()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    ret, next_frame = capture.read()\n",
    "    next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = next_frame.copy()\n",
    "\n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        break\n",
    "\n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = tf.convert_to_tensor(input_img)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    output_dict = model.signatures[\"serving_default\"](input_tensor)\n",
    "\n",
    "    classes = output_dict[\"detection_classes\"][0]\n",
    "    scores = output_dict[\"detection_scores\"][0]\n",
    "    boxes = output_dict[\"detection_boxes\"][0]\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    for idx, score in enumerate(scores):\n",
    "        if score > 0.7:\n",
    "            class_id = int(classes[idx])\n",
    "            box = boxes[idx]\n",
    "\n",
    "            x1 = int(box[1] * width)\n",
    "            y1 = int(box[0] * height)\n",
    "            x2 = int(box[3] * width)\n",
    "            y2 = int(box[2] * height)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), 255, 1)\n",
    "            cv2.putText(frame, classes_label[class_id] + \":\" + str(float(score)), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 255, 255), 1)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, flow=None, pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.1, flags=cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "    magnitude, angle = cv2.cartToPolar(flow[:, :, 0], flow[:, :, 1], angleInDegrees=True)\n",
    "    magnitude = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    angle = angle / 2\n",
    "\n",
    "    hsv = np.zeros_like(next_frame)\n",
    "    hsv[:, :, 0] = angle\n",
    "    hsv[:, :, 1] = magnitude\n",
    "    hsv[:, :, 2] = 255\n",
    "    frame = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    prev_gray = next_gray.copy()\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "    if cv2.waitKey(33) == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파네벡 광학 흐름 함수 적용 부분을 표시하기 위해 __극좌표 변환 함수__ 가 추가되었다.    \n",
    "- 해당 내용을 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파네벡 광학 흐름 함수는 프레임 내의 모든 픽셀에 대한 광학 흐름을 찾으므로 정확도는 높지만 연산량이 많다.    \n",
    "- 파네벡 함수는 검출 결과를 x와 y 성분으로 반환해 이미지 형태로 표시하기 어렵다.    \n",
    "- 하지만 __극좌표 변환 함수와 정규화 함수를 활용하면 좌표 성분을 벡터의 길이와 각도로 변환해 이미지 형태로 구성할 수 있다.__  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[직교좌표계와 극좌표계 : 이 블로그 정리가 아주 깔끔하네요!](https://ride-or-die.info/polar-coordinates/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 루카스 카나데 알고리즘       \n",
    "\n",
    "- 우리는 앞에서 파네벡 광학 흐름 함수를 살펴보았지만, 이 알고리즘은 특성상 __계산 과정이 복잡하고, 모든 픽셀에 대해 움직임을 계산하므로 cost가 많이 든다.__    \n",
    "- 이를 해결하기 위해 우리는 __영상에서 트래킹하기 쉬운 몇 개의 특징점을 검출해 해당 특징점에 대한 광학 흐름을 계산하는 희소 광학 흐름(Sparse optical flow) 알고리즘인 루카스 카나데 알고리즘을 알아본다.__   \n",
    "\n",
    "- 희소 광학 흐름은 __특징점에 대해서만 광학 흐름을 계산하므로, 연산시간이 짧고 신뢰도가 높지만__, __특징점을 제대로 검출하지 못할 때 정확성이 떨어진다는 단점이 있다.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 루카스 카나데 알고리즘은 기본적으로 다음의 세 가지 가정에 의해 광학 흐름을 판단한다.\n",
    "    1. 밝기 항상성(Brightness Constancy) :\n",
    "        - 프레임 내의 객체는 프레임 사이에서 움직이기 때문에 객체를 구성하는 픽셀은 변하지 않을 것이고, 따라서 그레이 스케일 이미지에서 트래킹을 시도할 때 픽셀의 밝기는 변하지 않는다.   \n",
    "    \n",
    "    2. 시간적 지속성(Temporal Coherence) :\n",
    "        - 프레임 내 객체의 움직임은 프레임 변화에 비해 느리게 움직인다. 프레임 간의 객체의 움직임은 거의 없어 상대적으로 객체는 움직임이 적다.       \n",
    "        \n",
    "    3. 공간적 지속성(Spatial Coherence) :      \n",
    "        - 프레임 내에서 인접한 픽셀들은 같은 객체로 볼 수 있으며 해당 픽셀은 동일한 움직임을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 루카스 카나데 알고리즘은 __윈도우에 대한 정보로 움직임을 검출__ 하기 때문에 희소 상황에서도 높은 검출율을 보일 수 있으나, 윈도우는 프레임에 비해 작은 크기를 가지므로 큰 움직임이 발생할 경우 윈도우 외부로 픽셀이 움직일 수도 있어 검색이 어려울 수도 있다.      \n",
    "- 윈도우 크기에 따른 불확실성을 감소시키기 위해, 이미지 피라미드를 활용한다. 가장 상단의 피라미드에서 광학 흐름을 계산하고 다음 피라미드에서 추정 결과를 사용한다. 이 연산을 가장 하단의 피라미드에 닿을 때까지 반복해 초기 가정을 개선한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q14. 루카스 카나데 함수를 파이썬 공식 가이드에서 찾아, 옵션을 설명해보세요.(cv2.calcOpticalFlowPyrLK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이전의 풀 버전 코드에서, 루카스카나데 광학 함수를 적용하기 위해 몇 줄을 추가해봅시다.\n",
    "# (주석처리)된 부분이 이전의 풀 버전과 비교하여 새로 추가된 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "with open(\"mscoco_complete_label_map.pbtxt\", \"rt\") as f:\n",
    "    pb_classes = f.read().rstrip(\"\\n\").split(\"\\n\")\n",
    "    classes_label = dict()\n",
    "\n",
    "    for i in range(0, len(pb_classes), 5):\n",
    "        pb_classId = int(re.findall(\"\\d+\", pb_classes[i + 2])[0])\n",
    "        pattern = 'display_name: \"(.*?)\"'\n",
    "        pb_text = re.search(pattern, pb_classes[i + 3])\n",
    "        classes_label[pb_classId] = pb_text.group(1)\n",
    "\n",
    "model = tf.saved_model.load(\"./ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model\")\n",
    "capture = cv2.VideoCapture(\"bird.mp4\")\n",
    "#ret, prev_frame = capture.read()\n",
    "#prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "#prev_pts = cv2.goodFeaturesToTrack(prev_gray, mask=None, maxCorners=1000, qualityLevel=0.1, minDistance=5, blockSize=9)\n",
    "\n",
    "while True:\n",
    "    #ret, next_frame = capture.read()\n",
    "    #next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n",
    "    #frame = next_frame.copy()\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray, mask=None, maxCorners=1000, qualityLevel=0.1, minDistance=5, blockSize=9)\n",
    "\n",
    "    if capture.get(cv2.CAP_PROP_POS_FRAMES) == capture.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "        break\n",
    "\n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    input_tensor = tf.convert_to_tensor(input_img)\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    output_dict = model.signatures[\"serving_default\"](input_tensor)\n",
    "\n",
    "    classes = output_dict[\"detection_classes\"][0]\n",
    "    scores = output_dict[\"detection_scores\"][0]\n",
    "    boxes = output_dict[\"detection_boxes\"][0]\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    for idx, score in enumerate(scores):\n",
    "        if score > 0.7:\n",
    "            class_id = int(classes[idx])\n",
    "            box = boxes[idx]\n",
    "\n",
    "            x1 = int(box[1] * width)\n",
    "            y1 = int(box[0] * height)\n",
    "            x2 = int(box[3] * width)\n",
    "            y2 = int(box[2] * height)\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), 255, 1)\n",
    "            cv2.putText(frame, classes_label[class_id] + \":\" + str(float(score)), (x1, y1 - 5), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 255, 255), 1)\n",
    "\n",
    "    if len(prev_pts) > 0:\n",
    "        next_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, next_gray, prev_pts, None)\n",
    "\n",
    "        for i, (prev_pt, next_pt) in enumerate(zip(prev_pts, next_pts)):\n",
    "            xp, yp = prev_pt.ravel()\n",
    "            xn, yn = next_pt.ravel()\n",
    "            \n",
    "    #prev_gray = next_gray.copy()\n",
    "    cv2.imshow(\"Object Detection\", frame)\n",
    "    if cv2.waitKey(33) == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q15. 위에서 주석처리된 함수는 어떤 기능을 하나요? 이 추가된 줄들은 어떤 역할을 하기 위해 추가되었나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 객체 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 활용한 방법은 텐서플로우를 활용해 객체를 검출하고, 인식하였다.    \n",
    "- 이제 OpenCV 함수로 객체를 인식해보자.   \n",
    "\n",
    "- __학습이 아닌 방법으로 객체를 인식하기 위해서는 특징점(Key Point)과 기술자(Descriptor)를 활용한다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특징점 : 영상에서, 배경과 구분될 수 있는 객체의 고유한 식별 지점    \n",
    "         서로 다른 이미지에서도 하나 이상의 점으로 특별하게 구분할 수 있는 작은 부분       \n",
    "        ex)해리스 코너     \n",
    "        \n",
    "- 기술자 : 서로 다른 이미지에서 특징점이 어떤 연관성을 가졌는지 구분하게 함 \n",
    "          기술자는 각 특징점이 가진 지역적 특징 정보를 가지고 있음     \n",
    "          서로 다른 특징점에서 차이를 구분해 특징점끼리 서로 매칭되게 함\n",
    "          \n",
    "          \n",
    "- 앞서 배운 __광학 흐름은 유사한 프레임 간에 특징점을 찾아 프름만을 계산하고, 객체의 구성요소나 식별 정보를 파악하지는 않는다.__     \n",
    "- 하지만 기술자는 유사한 이미지 뿐만 아니라 완전히 다른 이미지도 매칭할 수 있다.  \n",
    "- __기술자를 응용하면 완전히 다른 이미지나 빠른 프레임에서도 객체를 추적할 수 있게 된다.__       \n",
    "\n",
    "- OBR(Oriented FAST and rotated BRIEF) 알고리즘은 FAST(Feature from Accelerated Segment Test) 알고리즘, BRIEF(Binary Robust Independent Elementary Features) 알고리즘, 해리스 코너 알고리즘을 결합한 알고리즘이다.\n",
    "    1. FAST 알고리즘 :\n",
    "        - 픽셀 P와 픽셀 주변의 작은 원 위에 있는 픽셀의 집합을 비교하는 방식    \n",
    "        - 픽셀 P의 주변 픽셀에 임계갓을 적용해 어두운 픽셀, 밝은 픽셀, 유사한 픽셀로 분류해 원 위의 픽셀이 연속적으로 어둡거나, 밝아야 하며 이 연속성이 절반 이상이 돼야 한다. (이 조건을 만족한 경우 해당 픽셀은 우수한 특징점이다.)\n",
    "        \n",
    "    2. BRIEF 알고리즘(== 칼론더 피쳐) :\n",
    "        - 특징점을 검출하는 알고리즘이 아니라, 검출된 특징점에 대한 기술자를 생성하는데 사용\n",
    "        - 특징점 주변 영역의 픽셀을 다른 픽셀과 비교해 어느 부분이 더 밝은지를 찾아 이진 형식으로 저장. 가우시안 커널을 사용해 이미지를 컨벌루션 처리하고, 피쳐 중심의 가우스 분포를 통해 첫 번째 지점과 두번째 지점을 계산해 모든 픽셀을 한 쌍으로 생성한다. (두 개의 픽셀을 하나의 그룹으로 묶는 방식) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![패스트알고리즘](./PostingPic/10_fast.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[여기 읽어보시면 좋습니다!](https://darkpgmr.tistory.com/131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![briefobr](./PostingPic/10_briefobr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[파이썬 코드가 나와있습니다.](https://www.i2tutorials.com/what-is-orb-how-to-implement-orb-in-computer-vision/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ORB 알고리즘은 FAST 알고리즘을 통해 특징점을 검출한다.__       \n",
    "- FAST는 코너 뿐만 아니라 가장자리에도 반응하므로, 해리스코너 검출 알고리즘을 통해 최상위 특징점만 추출한다.     \n",
    "- 이 과정에서 이미지 피라미드를 구성해 스케일 공간 검색을 수행하고, 이 크기에 따라 피쳐 주변 박스 안의 강도 분포에 대해 X축과 Y축을 기준으로 1차 모멘트를 계산한다.   \n",
    "- 1차 모멘트는 그래디언트의 방향을 제공하므로 피처의 방향을 지정할 수 있고, 이를 통해 피쳐 벡터를 계산할 수 있다.      \n",
    "- 피쳐는 회전 불변성을 갖고 있으며 방향 정보를 포함하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q16. ORB 함수를 파이썬 공식 가이드에서 찾아, 옵션을 설명해보세요. \n",
    "(cv2.ORB_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q17. 특징점 및 기술자 계산 함수를 파이썬 공식 가이드에서 찾아, 옵션을 설명해보세요. \n",
    "(orb.detectAndCompute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q18. 전수조사 매칭 클래스를 파이썬 공식 가이드에서 찾아, 옵션을 설명해보세요. \n",
    "(cv2.BFMatcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q19. 각각의 함수와 클래스가 어떤 메커니즘으로 연결되어 있나요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q20. 아래의 코드는 ORB와 전수조사 매칭을 활용해 객체를 인식하는 코드입니다. 각각이 어떤 역할을 하는지, 주석을 달고 살펴보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"image.jpg\")\n",
    "img_object = cv2.imread(\"object.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "orb = cv2.ORB_create(nfeatures=40000)\n",
    "kp1, des1 = orb.detectAndCompute(gray, None)\n",
    "kp2, des2 = orb.detectAndCompute(img_object, None)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "count = 100\n",
    "for i in matches[:count]:\n",
    "    idx = i.queryIdx\n",
    "    x1, y1 = kp1[idx].pt\n",
    "    cv2.circle(img, (int(x1), int(y1)), 3, (0, 0, 255), 3)\n",
    "\n",
    "cv2.imshow(\"img\", img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘의 실습은 여기까지입니다.\n",
    "\n",
    "긴 실습 수행하시느라 고생하셨습니다! 토요일에 뵐게요 :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
