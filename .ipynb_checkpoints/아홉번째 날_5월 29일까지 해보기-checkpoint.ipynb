{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이번주 목표(노란책 Base의 openCV 머신러닝)               \n",
    "--- \n",
    "\n",
    "- 죄송합니다.. 얼룩말 책 개정판을 못빌렸네요 ㅠㅠㅠ....         \n",
    "- 얼룩말 책과 노란책의 목차와 서술을 참고하되, 내부 내용은 [python opencv machine learning 튜토리얼](https://docs.opencv.org/master/d6/de2/tutorial_py_table_of_contents_ml.html)을 참고하였습니다. \n",
    "- 이번 기회에 공식 문서를 보고 공부하는 방법을 연습해봐도 좋겠네요!\n",
    "\n",
    "### 목차\n",
    "\n",
    "0. 머신러닝 개요\n",
    "1. K-최근접 이웃 알고리즘               \n",
    "    - K-최근접 이웃 알고리즘       \n",
    "    - 적용해보기\n",
    "3. 서포트 벡터 머신      \n",
    "    - 서포트 벡터 머신 알고리즘     \n",
    "    - 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 머신러닝 개요          \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신러닝 : 주어진 데이터를 분석하여 규칙성, 패턴 등을 찾고, 이를 이용하여 의미있는 정보를 추출하는 과정을 말함.\n",
    "\n",
    "- 학습,훈련(train) : 데이터로부터 규칙성을 찾아내는 과정\n",
    "- 모델 : 학습, 훈련에 의해 결정된 규칙\n",
    "- 예측, 추론 : 새로운 데이터를 학습된 모델에 입력으로 전달하고 결과를 판단하는 과정          \n",
    "\n",
    "\n",
    "- 머신러닝은 크게 __지도학습과 비지도 학습으로 구분한다.__  \n",
    "- 지도학습 : 정답을 알고 있는 데이터를 이용하여 학습을 진행하는 방식    \n",
    "- 비지도학습\n",
    "\n",
    "- 아래의 사진은 지도학습 과정을 이미지로 나타낸 것이다. \n",
    "\n",
    "![머신러닝과정](./PostingPic/9_머신러닝과정.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영상 데이터는 픽셀로 이루어져 있지만, 이 픽셀 값을 그대로 머신러닝으로 사용하지는 않는다.\n",
    "\n",
    "#### Why?\n",
    "    - 영상 데이터는 조명 변화, 객체의 이동 및 회전 등에 매우 민감하게 반응하므로\n",
    "    \n",
    "#### So,\n",
    "    - 영상의 다양한 변환에도 크게 변경되지 않는 특징 정보를 추출하여 머신 러닝 입력으로 전달함\n",
    "    - ex) 사과와 바나나 사진을 구분한다고 할 때,\n",
    "        1. 영상의 주된 색상(hue채널)\n",
    "        2. 객체 외곽선(동그란 사과와 긴 타원형인 바나나)    \n",
    "        3. 면적 비율\n",
    "        \n",
    "    - 다수의 훈련 영상에서 특징 벡터를 추출하고, 이를 이용하여 머신 러닝 알고리즘을 학습시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지도학습은 주로 __회귀 또는 분류에 사용됨__\n",
    "- 회귀 : 연속된 수치 값을 예측하는 작업. ex)학생의 키와 몸무게를 학습한 뒤, 몸무게만 주었을 때 키 예측   \n",
    "- 분류 : 이산적인 값을 결과로 출력하는 머신러닝. ex) 사과와 바나나의 구분(사과-0클래스, 바나나-1클래스)   \n",
    "\n",
    "\n",
    "- 비지도학습은 훈련 데이터의 정답에 대한 정보 없이 오로지 데이터 자체만을 이용한 학습 방식    \n",
    "- 비지도 학습은 주로 __군집화에 사용__ 됨    \n",
    "\n",
    "\n",
    "#### 학습 후 테스트를 위해 영상의 모든 프레임을 학습에 사용하지는 않고, 일부는 분리하여 테스트로 사용하기도 한다.\n",
    "\n",
    "\n",
    "#### [파이썬을 활용한 openCV 머신러닝 튜토리얼](https://docs.opencv.org/master/d6/de2/tutorial_py_table_of_contents_ml.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-최근접 이웃 알고리즘                  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. KNN 알고리즘의 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K-최근접 이웃 알고리즘 : 분류 또는 회귀에 사용되는 지도학습 알고리즘       \n",
    "- K-최근접이 분류 태스크에 적용되면 : __특징 공간에서 테스트 데이터와 가장 가까운 k개의 훈련데이터를 찾고, K개의 훈련 데이터를 다수결로 판정하여 테스트 데이터의 클래스를 지정함__       \n",
    "- K-최근접이 회귀 태스크에 적용되면 : __테스트 데이터에 인접현 k개의 훈련 데이터의 평균을 테스트 데이터 값으로 설정__     \n",
    "\n",
    "![k-근접](./PostingPic/9_k최근접.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 공식 홈페이지에서 찾아본 KNN 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN in OpenCV\n",
    "We will do a simple example here, with two families (classes), just like above. Then in the next chapter, we will do an even better example.\n",
    "\n",
    "So here, we label the Red family as Class-0 (so denoted by 0) and Blue family as Class-1 (denoted by 1). We create 25 neighbours or 25 training data, and label each of them as either part of Class-0 or Class-1. We can do this with the help of a Random Number Generator from NumPy.\n",
    "\n",
    "Then we can plot it with the help of Matplotlib. Red neighbours are shown as Red Triangles and Blue neighbours are shown as Blue Squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q1. 아래의 코드를 실행시켜보고 위의 알고리즘을 바탕으로 의미를 분석해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature set containing (x,y) values of 25 known/training data\n",
    "trainData = np.random.randint(0,100,(25,2)).astype(np.float32)\n",
    "\n",
    "# Label each one either Red or Blue with numbers 0 and 1\n",
    "responses = np.random.randint(0,2,(25,1)).astype(np.float32)\n",
    "\n",
    "# Take Red neighbours and plot them\n",
    "red = trainData[responses.ravel()==0]\n",
    "plt.scatter(red[:,0],red[:,1],80,'r','^')\n",
    "\n",
    "# Take Blue neighbours and plot them\n",
    "blue = trainData[responses.ravel()==1]\n",
    "plt.scatter(blue[:,0],blue[:,1],80,'b','s')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 핸드라이팅 Digits을 분류하는 KNN알고리즘을 실행시켜 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OCR of Hand-written Digits\n",
    "\n",
    "Our goal is to build an application which can read handwritten digits. For this we need some training data and some test data. OpenCV comes with an image digits.png (in the folder opencv/samples/data/) which has 5000 handwritten digits (500 for each digit). Each digit is a 20x20 image. So our first step is to split this image into 5000 different digit images. Then for each digit (20x20 image), we flatten it into a single row with 400 pixels. That is our feature set, i.e. intensity values of all pixels. It is the simplest feature set we can create. We use the first 250 samples of each digit as training data, and the other 250 samples as test data. So let's prepare them first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q2. 아래의 소스코드를 실행시키거나, 직접 주석을 달아봅시다. (샘플 데이터는 opencv/samples/data 아래에 제공되고 있다고 명시하고 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('digits.png')\n",
    "gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Now we split the image to 5000 cells, each 20x20 size\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(gray,50)]\n",
    "\n",
    "# Make it into a Numpy array: its size will be (50,100,20,20)\n",
    "x = np.array(cells)\n",
    "\n",
    "# Now we prepare the training data and test data\n",
    "train = x[:,:50].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "test = x[:,50:100].reshape(-1,400).astype(np.float32) # Size = (2500,400)\n",
    "\n",
    "# Create labels for train and test data\n",
    "k = np.arange(10)\n",
    "train_labels = np.repeat(k,250)[:,np.newaxis]\n",
    "test_labels = train_labels.copy()\n",
    "\n",
    "# Initiate kNN, train it on the training data, then test it with the test data with k=1\n",
    "knn = cv.ml.KNearest_create()\n",
    "knn.train(train, cv.ml.ROW_SAMPLE, train_labels)\n",
    "ret,result,neighbours,dist = knn.findNearest(test,k=5)\n",
    "\n",
    "# Now we check the accuracy of classification\n",
    "# For that, compare the result with test_labels and check which are wrong\n",
    "matches = result==test_labels\n",
    "correct = np.count_nonzero(matches)\n",
    "\n",
    "accuracy = correct*100.0/result.size\n",
    "print( accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our basic OCR app is ready. This particular example gave me an accuracy of 91%. One option to improve accuracy is to add more data for training, especially for the digits where we had more errors.\n",
    "\n",
    "Instead of finding this training data every time I start the application, I better save it, so that the next time, I can directly read this data from a file and start classification. This can be done with the help of some Numpy functions like np.savetxt, np.savez, np.load, etc. Please check the NumPy docs for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "np.savez('knn_data.npz',train=train, train_labels=train_labels)\n",
    "# Now load the data\n",
    "with np.load('knn_data.npz') as data:\n",
    "    print( data.files )\n",
    "    train = data['train']\n",
    "    train_labels = data['train_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my system, it takes around 4.4 MB of memory. Since we are using intensity values (uint8 data) as features, it would be better to convert the data to np.uint8 first and then save it. It takes only 1.1 MB in this case. Then while loading, you can convert back into float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR을 영어 알파벳에 적용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will do the same for the English alphabet, but there is a slight change in data and feature set. Here, instead of images, OpenCV comes with a data file, letter-recognition.data in opencv/samples/cpp/ folder. If you open it, you will see 20000 lines which may, on first sight, look like garbage. Actually, in each row, the first column is a letter which is our label. The next 16 numbers following it are the different features. These features are obtained from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php). You can find the details of these features in [this page](http://archive.ics.uci.edu/ml/datasets/Letter+Recognition).\n",
    "\n",
    "There are 20000 samples available, so we take the first 10000 as training samples and the remaining 10000 as test samples. We should change the letters to ascii characters because we can't work with letters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "# Load the data and convert the letters to numbers\n",
    "data= np.loadtxt('letter-recognition.data', dtype= 'float32', delimiter = ',',\n",
    "                    converters= {0: lambda ch: ord(ch)-ord('A')})\n",
    "# Split the dataset in two, with 10000 samples each for training and test sets\n",
    "train, test = np.vsplit(data,2)\n",
    "# Split trainData and testData into features and responses\n",
    "responses, trainData = np.hsplit(train,[1])\n",
    "labels, testData = np.hsplit(test,[1])\n",
    "# Initiate the kNN, classify, measure accuracy\n",
    "knn = cv.ml.KNearest_create()\n",
    "knn.train(trainData, cv.ml.ROW_SAMPLE, responses)\n",
    "ret, result, neighbours, dist = knn.findNearest(testData, k=5)\n",
    "correct = np.count_nonzero(result == labels)\n",
    "accuracy = correct*100.0/10000\n",
    "print( accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습이 된 경우 정확도 93.22% 정도가 나올 것이라고 합니다. 정확도를 올리기 위해, 더 많은 데이터를 수집하여 적용해봅시다! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q3. Exercise : 여기서 우리는 k=5 로 설정했습니다. 만약 다른 K값을 사용하면 어떤 결과가 나올까요? 정확도를 극대화할 수 있는 k를 찾을 수 있을까요? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM(서포트 벡터 머신)          \n",
    "---\n",
    "\n",
    "### A. 서포트 벡터 머신 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 서포트 벡터 머신은 __두 개의 클래스로 구성된 데이터를 가장 여유있게 분리하는 초평면을 찾는 머신러닝 알고리즘이다.__   \n",
    "- ex) 2차원 공간상의 경우는 두 클래스의 데이터를 분리하는 직선 형태로, 3차원 공간의 경우는 3차원 공간에서의 평면의 방정식이 된다.\n",
    "\n",
    "- __SVM 알고리즘은 지도학습의 일종이며, 분류와 회귀에 사용된다.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm](./PostingPic/9_svm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a)그림에서, \n",
    "    - 직선 (1)과 (2)는 삼각형 사각형을 서로 잘 분리한다.      \n",
    "    - 하지만, 1번 직선은 왼쪽으로 조금만 움직여도 아슬아슬하고, 2번 직선은 오른쪽으로 조금만 움직여도 아슬아슬하다.\n",
    "    - 이를 잘 해결하기 위해 __그림 (b)처럼 적당한 간격을 가진 초평면 (3)번 직선을 찾으려는 것이 svm이다.__  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3번 직선에서 가장 가까운 빨간색/파란색 점과의 거리를 마진(margin)이라고 한다.\n",
    "- __SVM은 이 마진을 최대화하는 초평면을 구하는 알고리즘이다.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SVM 알고리즘은 기본적으로 선형으로 분리 가능한 데이터에 적용할 수 있다.    \n",
    "2. 하지만, 선형으로 분리되지 않는 경우(아래 그림의 a에 대항)\n",
    "    - SVM 알고리즘을 적용하기 위해 __커널 트릭__ 이라는 기법을 사용한다.\n",
    "    \n",
    "![커널트릭](./PostingPic/9_커널트릭.jpg)\n",
    "    \n",
    "#### 커널트릭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (a) 그림과 같이 2차원 평면상에서 분리가 어려운 경우, \n",
    "- (b) 그림처럼 데이터의 특징을 반영한 공간의 __차원을 증가시켜__ 데이터를 선형분리하는 것을 커널트릭이라 한다.\n",
    "\n",
    "- 아래의 표는 svm알고리즘에서 사용하는 커널 함수의 종류이다. \n",
    "\n",
    "![커널함수종류](./PostingPic/9_커널종류.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 위와 동일하게, 핸드라이팅 Digits을 분류하는 SVM을 적용해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[이 튜토리얼을 참고하였습니다.](https://docs.opencv.org/master/dd/d3b/tutorial_py_svm_opencv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR of Hand-written Digits\n",
    "\n",
    "In kNN, we directly used pixel intensity as the feature vector. This time we will use Histogram of Oriented Gradients (HOG) as feature vectors.\n",
    "- 이번에는 특징 벡터를 추출하기 위해 HOG를 사용합니다.\n",
    "\n",
    "Here, before finding the HOG, we deskew the image using its second order moments. So we first define a function deskew() which takes a digit image and deskew it. Below is the deskew() function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q4.아래의 코드를 한 번 분석해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deskew(img):\n",
    "    m = cv.moments(img)\n",
    "    if abs(m['mu02']) < 1e-2:\n",
    "        return img.copy()\n",
    "    skew = m['mu11']/m['mu02']\n",
    "    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n",
    "    img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![deskdew](./PostingPic/9_deskdew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 그림 중 왼쪽은 오리지널 이미지, 오른쪽은 deskew 함수를 적용한 이미지이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOG 함수 적용하기\n",
    "\n",
    "Next we have to find the HOG Descriptor of each cell. For that, we find Sobel derivatives of each cell in X and Y direction. Then find their magnitude and direction of gradient at each pixel. This gradient is quantized to 16 integer values. Divide this image to four sub-squares. For each sub-square, calculate the histogram of direction (16 bins) weighted with their magnitude. So each sub-square gives you a vector containing 16 values. Four such vectors (of four sub-squares) together gives us a feature vector containing 64 values. This is the feature vector we use to train our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog(img):\n",
    "    gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n",
    "    gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n",
    "    mag, ang = cv.cartToPolar(gx, gy)\n",
    "    bins = np.int32(bin_n*ang/(2*np.pi))    # quantizing binvalues in (0...16)\n",
    "    bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n",
    "    mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n",
    "    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n",
    "    hist = np.hstack(hists)     # hist is a 64 bit vector\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습에 적용하기\n",
    "\n",
    "- 위의 특징들을 가지고, 데이터셋에 학습을 적용해보자.\n",
    "\n",
    "[소스코드 풀버전](https://github.com/opencv/opencv/blob/master/samples/python/tutorial_code/ml/py_svm_opencv/hogsvm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "SZ=20\n",
    "bin_n = 16 # Number of bins\n",
    "affine_flags = cv.WARP_INVERSE_MAP|cv.INTER_LINEAR\n",
    "def deskew(img):\n",
    "    m = cv.moments(img)\n",
    "    if abs(m['mu02']) < 1e-2:\n",
    "        return img.copy()\n",
    "    skew = m['mu11']/m['mu02']\n",
    "    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n",
    "    img = cv.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n",
    "    return img\n",
    "def hog(img):\n",
    "    gx = cv.Sobel(img, cv.CV_32F, 1, 0)\n",
    "    gy = cv.Sobel(img, cv.CV_32F, 0, 1)\n",
    "    mag, ang = cv.cartToPolar(gx, gy)\n",
    "    bins = np.int32(bin_n*ang/(2*np.pi))    # quantizing binvalues in (0...16)\n",
    "    bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n",
    "    mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n",
    "    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n",
    "    hist = np.hstack(hists)     # hist is a 64 bit vector\n",
    "    return hist\n",
    "img = cv.imread(cv.samples.findFile('digits.png'),0)\n",
    "if img is None:\n",
    "    raise Exception(\"we need the digits.png image from samples/data here !\")\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(img,50)]\n",
    "# First half is trainData, remaining is testData\n",
    "train_cells = [ i[:50] for i in cells ]\n",
    "test_cells = [ i[50:] for i in cells]\n",
    "deskewed = [list(map(deskew,row)) for row in train_cells]\n",
    "hogdata = [list(map(hog,row)) for row in deskewed]\n",
    "trainData = np.float32(hogdata).reshape(-1,64)\n",
    "responses = np.repeat(np.arange(10),250)[:,np.newaxis]\n",
    "svm = cv.ml.SVM_create()\n",
    "svm.setKernel(cv.ml.SVM_LINEAR)\n",
    "svm.setType(cv.ml.SVM_C_SVC)\n",
    "svm.setC(2.67)\n",
    "svm.setGamma(5.383)\n",
    "svm.train(trainData, cv.ml.ROW_SAMPLE, responses)\n",
    "svm.save('svm_data.dat')\n",
    "deskewed = [list(map(deskew,row)) for row in test_cells]\n",
    "hogdata = [list(map(hog,row)) for row in deskewed]\n",
    "testData = np.float32(hogdata).reshape(-1,bin_n*4)\n",
    "result = svm.predict(testData)[1]\n",
    "mask = result==responses\n",
    "correct = np.count_nonzero(mask)\n",
    "print(correct*100.0/result.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 거의 94% 의 정확도가 나올 것이다. 다양한 파라미터를 적용하여, 더 높은 정확도가 나올 수 있도록 다양하게 트레이닝해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q5. OpenCV samples contain digits.py which applies a slight improvement of the above method to get improved result. It also contains the reference. Check it and understand it.\n",
    "\n",
    "opencv 샘플은 digits.py 파일을 포함하고 있습니다.. 이는 위의 결과보다 약간 더 향상된 결과를 내는 소스코드입니다. 이 샘플을 확인하고, 이해해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 오늘 실습은 여기까지입니다. \n",
    "\n",
    "###### 아무래도 책 내용이 정리된 것이 아니고, 단순히 openCV에서 제공하는 튜토리얼을 정리한 것이니 만큼 부족한 부분이 많을 것입니다. ㅠㅠㅠ 실습을 진행하시거나, 찬찬히 읽어보시면서 openCV에서의 머신러닝 활용에 대해 공부해보시고 토요일 오전에 뵙겠습니다!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
